# ü¶ô Installation d'Ollama sur Windows

## ‚ùå Erreur actuelle

```
ollama : Le terme ¬´ollama¬ª n'est pas reconnu comme nom d'applet de commande
```

Cela signifie qu'Ollama n'est pas install√© sur votre syst√®me Windows.

## ‚úÖ Installation d'Ollama

### Option 1 : Installation via le site officiel (Recommand√©)

1. **T√©l√©chargez Ollama pour Windows**
   - Allez sur : https://ollama.com/download
   - T√©l√©chargez le fichier d'installation pour Windows
   - Ou directement : https://ollama.com/download/windows

2. **Installez Ollama**
   - Ex√©cutez le fichier t√©l√©charg√© (`.exe`)
   - Suivez les instructions d'installation
   - Ollama sera install√© et ajout√© au PATH automatiquement

3. **V√©rifiez l'installation**
   ```powershell
   ollama --version
   ```

4. **T√©l√©chargez le mod√®le llama3.1**
   ```powershell
   ollama pull llama3.1
   ```
   ‚ö†Ô∏è **Attention** : Le t√©l√©chargement peut prendre plusieurs minutes (le mod√®le fait ~4.7 GB)

5. **V√©rifiez que le mod√®le est install√©**
   ```powershell
   ollama list
   ```

### Option 2 : Installation via winget (Windows Package Manager)

Si vous avez `winget` install√© :

```powershell
winget install Ollama.Ollama
```

Puis red√©marrez votre terminal et ex√©cutez :
```powershell
ollama pull llama3.1
```

## üöÄ D√©marrer Ollama

Apr√®s l'installation, Ollama devrait d√©marrer automatiquement. Sinon :

1. **Recherchez "Ollama"** dans le menu D√©marrer
2. **Lancez l'application Ollama**
3. **V√©rifiez que le service est actif** :
   ```powershell
   # Testez l'API
   curl http://localhost:11434/api/tags
   ```

## ‚öôÔ∏è Configuration dans Quran_back

Une fois Ollama install√©, v√©rifiez votre fichier `Quran_back/.env` :

```env
# Configuration Ollama
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=llama3.1
USE_OLLAMA=true

# MongoDB
MONGODB_URI=mongodb+srv://username:password@cluster.mongodb.net/quran-connect
PORT=3000
```

## üß™ Test rapide

Testez que tout fonctionne :

```powershell
# 1. V√©rifier qu'Ollama fonctionne
ollama run llama3.1 "Bonjour, comment √ßa va ?"

# 2. V√©rifier l'API
curl http://localhost:11434/api/generate -d "{\"model\": \"llama3.1\", \"prompt\": \"Bonjour\"}"
```

## üìù Alternative : D√©sactiver Ollama temporairement

Si vous ne voulez pas installer Ollama maintenant, vous pouvez d√©sactiver le LLM dans `Quran_back/.env` :

```env
USE_OLLAMA=false
```

Le chatbot utilisera alors des r√©ponses de fallback (moins intelligentes mais fonctionnelles).

## üîç D√©pannage

### Ollama ne d√©marre pas
- V√©rifiez que le service Ollama est en cours d'ex√©cution dans le Gestionnaire des t√¢ches
- Red√©marrez Ollama depuis le menu D√©marrer

### Le mod√®le ne se t√©l√©charge pas
- V√©rifiez votre connexion Internet
- Le t√©l√©chargement peut prendre du temps (4.7 GB pour llama3.1)
- Utilisez un mod√®le plus petit pour tester : `ollama pull llama3.2:1b` (plus petit, ~1.3 GB)

### Port 11434 d√©j√† utilis√©
- Fermez les autres instances d'Ollama
- Ou changez le port dans `Quran_back/.env` : `OLLAMA_URL=http://localhost:11435/api/generate`

